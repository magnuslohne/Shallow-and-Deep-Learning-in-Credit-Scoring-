---
title: "Random Forests"
---

This code is organised as follows: 
First, each model is trained (training time is calculated and displayed) and validation performance is computed.
Second, validation set performance is compared.
Third, the test set performance is computed and confusion matrix is displayed. 
Finally, the top ten most important features are identified and stored.


## Model 1

```{r}
start.time <- Sys.time()

set.seed(123)
RF_1 <- randomForest(as.factor(Default_next_12_months) ~ ., data = train_set,
mtry = 4, importance = TRUE, do.trace = 100, n_tree=100)

end.time <- Sys.time()
time.taken <- end.time - start.time
print(time.taken)

rf_prob_pred1 <- predict(RF_1, newdata=validation_set, "prob")[1:nrow(validation_set)]
rf_prob_pred1 <- 1 - rf_prob_pred1 #flipped probabilities

val_metrics_rf1 <- perf_met_fu(rf_prob_pred1, actual_class_validation)
```



## Model 2

```{r}
set.seed(123)
start.time <- Sys.time()

RF_2 <- randomForest(as.factor(Default_next_12_months) ~ ., data = train_set,
mtry = 4, importance = TRUE, do.trace = 100, n_tree=200)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

rf_prob_pred2 <- predict(RF_2, newdata=validation_set, "prob")[1:nrow(validation_set)]
rf_prob_pred2 <- 1 - rf_prob_pred2
val_metrics_rf2 <- perf_met_fu(rf_prob_pred2, actual_class_validation)
```



## Model 3

```{r}
set.seed(123)
class_weights <- c(`0` = 1, `1` = 50) #use weights to put emphasis on minority class

start.time <- Sys.time()

RF_3 <- randomForest(as.factor(Default_next_12_months) ~ ., data = train_set,
mtry = 6, importance = TRUE, do.trace = 100, n_tree=100, classwt = class_weights)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

rf_prob_pred3 <- predict(RF_3, newdata=validation_set, "prob")[1:nrow(validation_set)]
rf_prob_pred3 <- 1 - rf_prob_pred3
val_metrics_rf3 <- perf_met_fu(rf_prob_pred3, actual_class_validation)
```



## Model 4

```{r}
class_weights <- c(`0` = 1, `1` = 20) # new class weights

start.time <- Sys.time()

set.seed(123)
RF_4 <- randomForest(as.factor(Default_next_12_months) ~ ., data = train_set,
mtry = 6, importance = TRUE, do.trace = 100, n_tree=100, classwt = class_weights)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

rf_prob_pred4 <- predict(RF_4, newdata=validation_set, "prob")[1:nrow(validation_set)]
rf_prob_pred4 <- 1 - rf_prob_pred4
val_metrics_rf4 <- perf_met_fu(rf_prob_pred4, actual_class_validation)
```



## Comparison on validation set

```{r}
models <- c("RF1", "RF2", "RF3", "RF4")

results_tableRF <- data.frame(
  Model = models,
  AUC = c(val_metrics_rf1[1], val_metrics_rf2[1], val_metrics_rf3[1], val_metrics_rf4[1]),
  PR_AUC = c(val_metrics_rf1[2], val_metrics_rf2[2], val_metrics_rf3[2], val_metrics_rf4[2]),
  Brier_Score = c(val_metrics_rf1[3], val_metrics_rf2[3], val_metrics_rf3[3], val_metrics_rf4[3])
)

print(results_tableRF)
```



## Performance on test set
```{r}
test_prob_pred_RF <- predict(RF_1, newdata=test_set, "prob")[1:nrow(test_set)]
test_prob_pred_RF <- 1 - test_prob_pred_RF

test_metrics_RF <- perf_met_fu(test_prob_pred_RF, actual_class_test)
print(paste0("ROC-AUC: ", test_metrics_RF[1], ", PR-AUC: ", test_metrics_RF[2], ", Brier score: ", test_metrics_RF[3]))

optimal_threshold_RF <- opt_thres_fu(test_prob_pred_RF, actual_class_test)
test_class_pred_RF <- ifelse(test_prob_pred_RF > as.numeric(optimal_threshold_RF)[1], 1, 0)

confusion_matrix_RF <- cm_fu(test_class_pred_RF, actual_class_test)
print(confusion_matrix_RF)
```



## Interpretability/Variable importance

```{r}
var_imp_RF <- as.data.frame(randomForest::importance(RF_1))[,c(3,4)]
var_imp_RF <- head(var_imp_RF[order(var_imp_RF$MeanDecreaseGini, decreasing = TRUE), ], 10)

print(var_imp_RF)
```
